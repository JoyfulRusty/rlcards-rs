TODO:
    提高基于梯度的优化方法(如随机梯度下降(SGD)及其变体)的收敛速度和泛化性能。
    Lookahead背后的思想是在当前梯度更新的方向上迈出一步，然后使用一组额外的权重(称为“慢权重”)在同一方向上迈出一步，但时间范围更长。
    与原始权重相比，这些慢权重更新的频率更低，有效地创建了对优化过程未来的“展望”。


在训练期间，Lookahead计算两个权重更新:
快速权重更新，它基于当前的梯度并应用于原始权重，以及慢速权重更新，它基于之前的慢速权重并应用于新的权重集。
这两个更新的组合给出了最终的权重更新，用于更新原始权重。使用慢权重提供了一种正则化效果，有助于防止过拟合并提高泛化性能。
此外，这种前瞻机制有助于优化器更有效地逃避局部最小值和鞍点，从而导致更快的收敛。



TODO:
    Radam优化器是对adam优化器的升级版，
    这个优化器是在2019年的论文《ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND》中提出的一种方法。
    这种方法相对于Adam优化器在训练速度和效率上均有所提高